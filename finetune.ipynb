{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatting..: 100%|███████████████████| 24739/24739 [00:00<00:00, 176714.53it/s]\n"
     ]
    }
   ],
   "source": [
    "!python cover_military2jsonl.py \\\n",
    "    --data_path data/military.json \\\n",
    "    --save_path data/military.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tokenize_dataset_rows.py \\\n",
    "    --jsonl_path data/military.jsonl \\\n",
    "    --save_path data/military \\\n",
    "    --max_seq_length 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dataQ/zhijian/anaconda3/envs/chatglm did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('{\"locale\"'), PosixPath('true}'), PosixPath('\"/dataQ/zhijian/.vscode-server-insiders/data/clp/0f47626ed20b541b4f5b69d0ee0ddde7.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/dataQ/zhijian/.vscode-server-insiders/data/clp/0f47626ed20b541b4f5b69d0ee0ddde7.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"/dataQ/zhijian/.vscode-server-insiders/data/clp/0f47626ed20b541b4f5b69d0ee0ddde7.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/dataQ/zhijian/.vscode-server-insiders/data/clp/0f47626ed20b541b4f5b69d0ee0ddde7.zh-cn/02298ef4180f2a5db39e0cb6288382860b96a596\",\"_corruptedFile\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('{\"*\"'), PosixPath('\"0f47626ed20b541b4f5b69d0ee0ddde7.zh-cn\",\"_translationsConfigFile\"')}\n",
      "  warn(msg)\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/Users/dingzhijian/.vscode-insiders/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.79.2023051709/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/dataQ/zhijian/LLMs/chatglm-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/dataQ/zhijian/LLMs/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test before finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/site-packages/transformers/generation/utils.py:1445: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 战斗机涡轮前温度有什么用?\n",
      "Answer: 战斗机涡轮前温度(涡轮进气道温度)是一个重要的飞行参数,通常被用于评估涡轮发动机的性能和可靠性。\n",
      "\n",
      "涡轮发动机通过吸入空气来产生动力,其中一部分空气被冷却并在涡轮前通过。涡轮前温度是一个重要的指标,反映了涡轮发动机对冷却空气的处理能力。较高的涡轮前温度通常意味着涡轮发动机具有更强的处理能力,但同时也意味着需要更多的冷却空气来保持温度稳定。\n",
      "\n",
      "在战斗机飞行中,涡轮前温度的测量和优化可以帮助发动机在高速飞行时保持稳定性和可靠性,从而提高战斗机的性能和飞行效率。因此,战斗机设计者通常会通过测量和优化涡轮前温度来优化发动机的性能和飞行特性。\n",
      "### 1.Answer:\n",
      " 原则上前涡轮温度升高后，燃烧更加充分，推力有所上升。但单纯增加涡轮前温会导致风扇寿命下降，WS10就出现过这种问题。 \n",
      "\n",
      "\n",
      "Instruction: 上游二号是超音速反舰导弹吗\n",
      "Answer: 上游二号是中国研制的一种超音速反舰导弹。虽然上游二号没有公开具体的飞行速度数据,但据报道,它采用高超音速飞行,具有较远的射程和较强的反舰能力。\n",
      "### 2.Answer:\n",
      " 不是，还是高亚音速的。上游2还是冥河的老底子，看看那个气动外形——卵型。就知道了，没法飞超音速。 \n",
      "\n",
      "\n",
      "Instruction: 哪个国家激光武器最先进\n",
      "Answer: 当今世界上最先进的激光武器国家之一是俄罗斯。俄罗斯在激光武器领域有着悠久的历史和深厚的积累,已经开发出了一系列先进的激光武器系统,如“瓦良格”号航母上使用的激光炮、苏-35战斗机使用的激光器等。\n",
      "\n",
      "美国也是激光武器领域的领先国家之一,其军队已经装备了一系列先进的激光武器系统,如“激光器家族”、激光照射器等。英国、法国、中国、日本等其他国家也在不同程度上进行了激光武器的研发和应用。\n",
      "\n",
      "需要注意的是,激光武器是一种高度技术化的兵器,其性能往往受到多种因素的影响,如激光器的功率、准确度、稳定性等,因此不同国家在激光武器\n",
      "### 3.Answer:\n",
      " 激光武器是一种利用定向发射的激光束直接毁伤目标或使之失效的定向能武器。武器系统主要由激光器和跟踪、瞄准、发射装置等部分组成。目前通常采用的激光器有化学激光器、固体激光器、二氧化碳激光器等。激光武器具有攻击速度快、转向灵活、可实现精确打击、不受电磁干扰等优点，但也存在昴受天气和环境影响等弱点。目前，激光器的种类繁多，名称各异，有体积整整占据一幢大楼、功率为上万亿瓦、用于引发核聚变的激光器，也有比人的指甲还小、输出功率仅有几毫瓦、用于光电通信的半导体激光器。按工作介质区分，可分为固体激光器、液体激光器和分子型、离子型、准分子型的气体激光器；按发射位置可分为天基、陆基、舰载、车载和机载等类型；按用途可分为战术型和战略型两类。打字不易，如满意。 \n",
      "\n",
      "\n",
      "Instruction: 火炮的炮弹落点是靠计算抛物线吗?\n",
      "Answer: 火炮的炮弹落点通常靠计算抛物线来确定。炮弹在飞行过程中,受到重力、空气阻力和弹道系数等因素的影响,其轨迹可以近似看作一个抛物线。因此,火炮工程师需要通过计算抛物线的顶点和横截面积,来确定炮弹在目标区域中的落点。这种方法可以有效地减少误差,确保炮弹能够准确地落在目标点上。\n",
      "### 4.Answer:\n",
      " 是的，通过计算空气风速，湿度，与目标的距离，测算出射击诸元，但是精准度不高的原因是火炮自身的制造精度不高，与特殊的风向风力的改变。导弹是有制导装置的，比如雷达制导，红外自主制导。 \n",
      "\n",
      "\n",
      "Instruction: 美国超级脂肪燃烧弹的效果好不好 ?\n",
      "Answer: 超级脂肪燃烧弹是一种通过物理手段促进脂肪燃烧的药物,其效果取决于多种因素,包括药物的质量、使用方法、个体差异等。\n",
      "\n",
      "超级脂肪燃烧弹通常含有咖啡因、辣椒素等成分,能够刺激神经系统,提高身体代谢率,促进脂肪燃烧。但是,由于每个人的代谢率、身体状况和使用方法不同,因此超级脂肪燃烧弹的效果可能因人而异。\n",
      "\n",
      "一些研究表明,超级脂肪燃烧弹在一定程度上能够减少脂肪量、降低体重和改善脂肪燃烧率。但是,超级脂肪燃烧弹的效果可能受到多种因素的影响,因此需要在使用前咨询医生或专业营养师,确保使用方法和剂量正确,并根据个人情况进行个性化的调整。\n",
      "### 5.Answer:\n",
      " 只有带美国日本韩国的物品尽量少用。我自己都用国内的。你是要减肥吗/。建议用绿瘦! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cover2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"data/military.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_path = \"data/military/\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "train_num = 500\n",
    "\n",
    "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, HfArgumentParser\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "class ModifiedTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 07:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.787700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.673600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.611100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=1.4622032979329427, metrics={'train_runtime': 474.9934, 'train_samples_per_second': 3.158, 'train_steps_per_second': 3.158, 'total_flos': 3781851053211648.0, 'train_loss': 1.4622032979329427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16 =True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    learning_rate = 1e-4,\n",
    "    max_steps=1500,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=mini_train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test After finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2636336/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1101185756.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2636336/1101185756.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/json/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">293</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">kwarg; otherwise ``JSONDecoder`` is used.</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>293 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loads(fp.read(),                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>, object_hook=object_hook,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">295 │   │   </span>parse_float=parse_float, parse_int=parse_int,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 │   │   </span>parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/encodings/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ascii.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">26</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">IncrementalDecoder</span>(codecs.IncrementalDecoder):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, final=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>):                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>26 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> codecs.ascii_decode(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.errors)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">StreamWriter</span>(Codec,codecs.StreamWriter):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">pass</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">UnicodeDecodeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ascii'</span> codec can't decode byte <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0xe6</span> in position <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span>: ordinal not in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">range</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2636336/\u001b[0m\u001b[1;33m1101185756.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2636336/1101185756.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/json/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m293\u001b[0m in \u001b[92mload\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mkwarg; otherwise ``JSONDecoder`` is used.\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m293 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m loads(fp.read(),                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mcls\u001b[0m=\u001b[96mcls\u001b[0m, object_hook=object_hook,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   \u001b[0mparse_float=parse_float, parse_int=parse_int,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   │   \u001b[0mparse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/dataQ/zhijian/anaconda3/envs/chatglm/lib/python3.10/encodings/\u001b[0m\u001b[1;33mascii.py\u001b[0m:\u001b[94m26\u001b[0m in \u001b[92mdecode\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mIncrementalDecoder\u001b[0m(codecs.IncrementalDecoder):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecode\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m, final=\u001b[94mFalse\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m26 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m codecs.ascii_decode(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.errors)[\u001b[94m0\u001b[0m]                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mStreamWriter\u001b[0m(Codec,codecs.StreamWriter):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mpass\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mUnicodeDecodeError: \u001b[0m\u001b[32m'ascii'\u001b[0m codec can't decode byte \u001b[1;36m0xe6\u001b[0m in position \u001b[1;36m62\u001b[0m: ordinal not in \u001b[1;35mrange\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cover2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"data/military.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_tunable_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(\"cpu\")\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "\n",
    "save_tunable_parameters(model, os.path.join(\"output\", \"chatglm-lora.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
